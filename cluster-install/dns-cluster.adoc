= Install DNS-based Kubernetes cluster using Kops
:toc:

This tutorial will walk you through how to install a Kubernetes cluster using kops. This cluster will use DNS for node discovery and communication.

https://github.com/kubernetes/kops[Kops], short for Kubernetes Operations, is a set of tools for installing, operating, and deleting Kubernetes clusters in the cloud. A rolling upgrade of an older version of Kubernetes to a new version can also be performed. It also manages the cluster add-ons. After the cluster is created, the usual kubectl CLI can be used to manage resources in the cluster.

== Install kops and kubectl

There is no need to download the Kubernetes binary distribution for creating a cluster using kops. However, you do need to download the kops CLI. It then takes care of downloading the right Kubernetes binary in the cloud, and provisions the cluster.

    brew update && install kops
    curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.goo\
    gleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl
    chmod +x ./kubectl
    sudo mv ./kubectl /usr/local/bin/kubectl

== IAM user permission

Make sure the latest version of http://docs.aws.amazon.com/cli/latest/userguide/installing.html[AWS CLI]
is installed. User permissions being used must have these http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies.html[IAM policies] attached

    AmazonEC2FullAccess
    AmazonRoute53FullAccess
    AmazonS3FullAccess
    IAMFullAccess
    AmazonVPCFullAccess

Please review this link for additional info on IAM permission:
https://github.com/kubernetes/kops/blob/master/docs/aws.md#setup-iam-user

== S3 bucket to store Kubernetes config

Kops needs a “state store” to store configuration information of the cluster.  For example, how many nodes, instance type of each node, and Kubernetes version. The state is stored during the initial cluster creation. Any subsequent changes to the cluster are also persisted to this store as well. As of now, Amazon S3 is the only supported storage mechanism. Create a S3 bucket and pass that to the kops CLI during cluster creation.

    aws s3api create-bucket --bucket kubernetes-aws-config
    # enable versioning and export
    aws s3api put-bucket-versioning \
      --bucket kubernetes-aws-config 
      --versioning-configuration\
    Status=Enabled
    export KOPS_STATE_STORE=s3://kubernetes-aws-config

== Create hosted zone on Route53

A top-level domain or a subdomain is required to create the cluster. This domain allows the worker nodes to discover the master and the master to discover all the etcd servers. This is also needed for kubectl to be able to talk directly with the master.

This domain may be registered with AWS, in which case a Route 53 hosted zone is created for you. Alternatively, this domain may be at a different registrar. In this case, create a Route 53 hosted zone. Specify the name server (NS) records from the created zone as NS records with the domain registrar.

If you own Route53 domain name, download https://github.com/stedolan/jq/wiki/Installation[jq]
and run this command:

    ID=$(uuidgen) && aws route53 create-hosted-zone \
      --name kubernetes-aws.io \
      --caller-reference $ID \
      | jq .DelegationSet.NameServers

See other means of creating DNS: https://github.com/kubernetes/kops/blob/master/docs/aws.md#configure-dns

If you don't own a DNS domain name, you can create custom domain using Route53's private hosted zone
you need to provide VPC info to run this command. See below how to <<Create VPC (Optional)>>

    ID=$(uuidgen) && aws route53 create-hosted-zone ]\
      --name k8s-aws.internal \
      --vpc VPCRegion=us-east-1,VPCId=$VPCID \
      --caller-reference $ID \
      | jq .DelegationSet.NameServers

== Create kubernetes cluster

The Kops CLI can be used to create a highly available cluster, with multiple master nodes spread across multiple Availability Zones. Workers can be spread across multiple zones as well. Some of the tasks that happen behind the scene during cluster creation are:

- Provisioning EC2 instances
- Setting up AWS resources such as networks, Auto Scaling groups, IAM users, and security groups
- Installing Kubernetes.

Below are examples of how to use kops to create different cluster configurations.

=== Create a single master, multi-node, multi-az cluster
Create a Kubernetes cluster using the following command. This will create a cluster with a single master, multi node and multi-az configuration:

    kops create cluster \
      --name cluster01.kubernetes-aws.io \
      --zones us-east-1d,us-east-1e \
      --state s3://kubernetes-aws-config

Now build the cluster:

    # review cluster info and make changes if necessary (for e.g., subnet sizing)
    kops get cluster
    kops edit cluster cluster01.kubernetes-aws.io
    # update and commit
    kops update cluster cluster01.kubernetes-aws.io --yes

=== Create a multi-master, multi-node, multi-az cluster
Create a cluster with multi-master, multi-node and multi-az configuration

    kops create cluster \
      --name cluster02.kubernetes-aws.io \
      --master-count 3 \
      --master-zones us-east-1a,us-east-1b,us-east-1c \
      --node-count 5 \
      --zones us-east-1a,us-east-1b,us-east-1c \
      --state s3://kubernetes-aws-config

Now build the cluster:

    # review cluster info and make changes if necessary (for e.g., subnet sizing)
    kops get cluster
    kops edit cluster cluster02.kubernetes-aws.io
    # update and commit
    kops update cluster cluster02.kubernetes-aws.io --yes

=== Create a multi-master, multi-node, multi-az cluster with a Route53 private hosted zone and VPC

    kops create cluster \
      --dns private \
      --name cluster03.k8s-aws.internal \
      --master-zones us-east-1a,us-east-1b,us-east-1c \
      --node-count 6 \
      --zones us-east-1a,us-east-1b,us-east-1c \
      --state s3://kubernetes-aws-config \
      --vpc $VPCID \
      --network-cidr 10.1.0.0/16\
      --ssh-public-key $mypubkey

Now build the cluster:

    # review cluster info and make changes if necessary (for e.g., subnet sizing)
    kops get cluster
    kops edit cluster cluster03.k8s-aws.internal
    # update and commit
    kops update cluster cluster03.k8s-aws.internal --yes

== Validate the cluster

    kops validate cluster

The following is the output for a cluster with 3 master nodes and 6 worker nodes using a Route53 private hosted zone

    Using cluster from kubectl context: cluster03.k8s-aws.internal
    Validating cluster cluster03.k8s-aws.internal
    INSTANCE GROUPS
    NAME			ROLE	MACHINETYPE	MIN	MAX	SUBNETS
    master-us-east-1a-1	Master	m3.medium	1	1	us-east-1a
    master-us-east-1b-1	Master	m3.medium	1	1	us-east-1b
    master-us-east-1c-1	Master	m3.medium	1	1	us-east-1c
    nodes			Node	t2.medium	6	6	us-east-1a,us-east-1b,us-east-1c

    NODE STATUS
    NAME				ROLE	READY
    ip-10-10-105-101.ec2.internal	node	True
    ip-10-10-127-80.ec2.internal	node	True
    ip-10-10-33-192.ec2.internal	master	True
    ip-10-10-36-230.ec2.internal	master	True
    ip-10-10-45-69.ec2.internal	node	True
    ip-10-10-51-111.ec2.internal	node	True
    ip-10-10-71-96.ec2.internal	node	True
    ip-10-10-87-59.ec2.internal	node	True
    ip-10-10-93-160.ec2.internal	master	True
    Your cluster cluster03.k8s-aws.internal is ready

TIP: You may need to add cluster API endpoints into your hosts file (/etc/hosts) if you use a Route53
private hosted zone along with the VPC option.

== Create VPC (Optional)

     VPCID=`aws ec2 create-vpc --cidr-block 10.1.0.0/16 --region us-east-1 --query 'Vpc.VpcId' --output text`
     # modify dns hostname resolution for the VPC
     aws ec2 modify-vpc-attribute --vpc-id $VPCID --enable-dns-hostnames "{\"Value\":true}"
     # create internet gateway and attach it to VPC
     IGW=`aws ec2 create-internet-gateway --region us-east-1 --query 'InternetGateway.InternetGatewayId' --output text`
     aws ec2 attach-internet-gateway --internet $IGW --vpc $VPCID --region us-east-1

== Delete cluster

    kops delete cluster \
      cluster03.k8s-aws.internal \
      --state s3://kubernetes-aws-config \
      --yes
    # Find Route53 hosted zone ID from the console or via CLI and delete hosted zone
    aws route53 delete-hosted-zone --id Z1234567890ABC
    # Delete VPC if you created earlier
    aws ec2 detach-internet-gateway --internet $IGW --vpc $VPCID --region us-east-1
    aws ec2 delete-internet-gateway --internet-gateway-id $IGW
    aws ec2 delete-vpc --vpc-id $VPCID

