
= Kubernetes - App Scaling
:icons:
:linkcss:
:imagesdir: ../images

== Kubernetes - App Scaling

Applications can be scaled out to additional replicas in response to resource utilization metrics.

Kubernetes leverages Heapster for the  monitored by HeapsKubernetes There are many options for aggregating and streaming kubernetes logs. In this example we are going to focus on an AWS based deployment which consists of the following:

1. Fluentd DaemonSet deployed with the Cloud Watch Logs plugin
2. Logs will stream to a CloudWatch Log Group
3. Lambda will stream the Log Group into a Amazon Elasticsearch cluster
=== Prerequisistes

1. Deploy a Kubernetes cluster
2. Deploy Heapster
2. Deploy an Application - pick an application that would be simple to generate lod

=== Horizontal Pod Autoscaler Configuration
    kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10

=== Generate Load

The simplest method to do this would be to access the application in an infinite loop as documented on the Kubernetes site

    / # while true; do wget -q -O- 100.96.2.4; done
    kubectl attach load-generator-3044827360-9tf2w -c load-generator -i -t


3. Configure an autoscaler for the application
Create a Horizontal Pod Autoscaling
=== Configure CloudWatch Log Group

    aws logs create-log-group --log-group-name kubernetes-logs --region us-east-1

=== Prepare Kubernetes manifest files

If you customized the log_group_name (from the previous command) or want to use a custom log_stream_name, you will need to update the output.conf section in ../templates/fluent-configmap.yaml

    output.conf: |
      <match **>
        # Plugin specific settings
        type cloudwatch_logs
        log_group_name kubernetes-cluster
        log_stream_name fluentd-cloudwatch-demo
        auto_create_stream true
        # Buffer settings
        buffer_chunk_limit 2M
        buffer_queue_limit 32
        flush_interval 10s
        max_retry_wait 30
        disable_retry_limit
        num_threads 8
      </match>

You will need to create an IAM user and set the AWS_ACCESS_KEY, AWS_SECRET_KEY and AWS_REGION in the ../templates/fluentd-ds.yaml file. Other options to accomplish this (in a production environment) are to use either kubernetes secrets or ec2 IAM roles.

  env:
  - name: FLUENTD_CONFIG
    value: fluentd-standalone.conf
  - name: AWS_REGION
    value: $REGION
  - name: AWS_ACCESS_KEY
    value: $ACCESS_KEY
  - name: AWS_SECRET_KEY
    value: $SECRET_KEY

=== Deploy Fluentd into Kubernetes

First create the logging namespace

    kubectl create ns logging

Create all of the necessary service accounts and roles:

    kubectl create -f ./templates/fluentd-service-account.yaml
    kubectl create -f ./templates/fluentd-role.yaml
    kubectl create -f ./templates/fluentd-role-binding.yaml

Then deploy Fluentd:

    kubectl create -f ./templates/fluentd-configmap.yaml
    kubectl create -f ./templates/fluentd-svc.yaml
    kubectl create -f ./templates/fluentd-ds.yaml

Watch for all of the pods to change to running status:

    kubectl get pods --watch --namespace=logging

We can now login to the AWS console -> Management Tools -> CloudWatch -> Logs -> kubernetes-cluster -> fluentd-cloudwatch-demo

We should start to see logs arrive into the service and can use the search feature to looks for specific logs

=== Provision an Amazon Elasticsearch cluster

This example creates a two instance Amazon Elasticsearch cluster named kops-logs in the Virgina region. Note that this cluster has an open access policy which will need to be locked down in production environments:

    aws es create-elasticsearch-domain --domain-name kubernetes-logs --elasticsearch-version 5.5 --elasticsearch-cluster-config  InstanceType=m4.large.elasticsearch,InstanceCount=2 --ebs-options EBSEnabled=true,VolumeType=standard,VolumeSize=100 --access-policies '{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"AWS":["*"]},"Action":["es:*"],"Resource":"*"}]}' --region us-east-1

Follow the instructions here to stream your logs from CloudWatch logs to your Amazon Elasticsearch cluster:

http://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html

Now you can go to the Amazon Elasticsearch console and open the Kibana dashboard from the link and begin using the capabilites to search and visuzalize your Kubernetes cluster metrics

== Conclusion

In this post we demonstrated how to leverage AWS managed services to collect, search and visualize your kubernetes metrics. This can be used as reference to being to build your own logging solution for Kubernetes on top of AWS.
